{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training Notebook\n",
    "\n",
    "This notebook is for training the final model, after finding appropriate hyperparameters. Much of this code is copied from the dev notebook as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten, Lambda, BatchNormalization\n",
    "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_json, make_logger\n",
    "import logging\n",
    "import itertools\n",
    "import pickle\n",
    "import difflib\n",
    "\n",
    "params = load_json('params.json')\n",
    "logger = make_logger('train', 'log/train.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test sets and images\n",
    "\n",
    "Creates a dict relating artist MBIDs to spectrogram images, represented as 128x128x1 arrays of floats. Also load the previously saved database of artist relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391278ba0ef34241921eb1b781d294af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2791), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading images')\n",
    "\n",
    "images = defaultdict(list)\n",
    "\n",
    "for path in tqdm_notebook(glob.glob('tracks/**/*.png', recursive=True)):\n",
    "    mbid = os.path.basename(path).rsplit('-', 1)[0]\n",
    "    img = np.rot90(np.array(Image.open(path).convert('L')) / 255)\n",
    "    #img = np.reshape(img, img.shape + (1,))\n",
    "    images[mbid].append(img)\n",
    "    \n",
    "logger.info('Loaded images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training set\n",
      "Loaded training set\n",
      "Loading test set\n",
      "Loaded test set\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading training set')\n",
    "\n",
    "train_set = pd.read_hdf('dataset/train_min.hd5', key='artists')\n",
    "\n",
    "logger.info('Loaded training set')\n",
    "\n",
    "logger.info('Loading test set')\n",
    "\n",
    "test_set = pd.read_hdf('dataset/test_min.hd5', key='artists')\n",
    "\n",
    "logger.info('Loaded test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Artists in images: 975\n",
      "Training set length: 877\n",
      "Test set length: 975\n"
     ]
    }
   ],
   "source": [
    "logger.info('Artists in images: {}'.format(len(images)))\n",
    "logger.info('Training set length: {}'.format(len(train_set)))\n",
    "logger.info('Test set length: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define accuracy metric\n",
    "\n",
    "For my accuracy metric, I get the top similar artists for each artist from the original dataset, then to calculate accuracy of the model, I use the model to calculate all pairwise similarity scores and get the top predicted similar artists. Then I compare the two using edit distance, and return the average value as an accuracy.\n",
    "\n",
    "Calculating accuracy like this is slow because you pretty much have to go through a full epoch. It's probably only useful at the end of training.\n",
    "\n",
    "There are two accuracy metrics I use, all of which mean slightly different things:\n",
    "\n",
    "`compare_accuracy_edit_dist` compares the top n artists in each dataframe by edit distance, and returns it as a proportion to the maximum edit distance possible (i.e. n)\n",
    "\n",
    "`compare_accuracy_unordered` compares how many of the top n artists are shared between both dataframes, disregarding order, as a proportion of n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_to_top(matrix):\n",
    "    # Converts a similarity score matrix to a sorted table of most similar artists\n",
    "    most_similar = np.argsort(-matrix.values, axis=1)\n",
    "    return pd.DataFrame(most_similar, index=matrix.index).T\n",
    "\n",
    "def eval_artist_similarity(artist_A, artist_B, model):\n",
    "    # Collect all pairwise combinations of artist tracks, evaluates the model on them, then\n",
    "    # returns the similarity as an average of all the predictions\n",
    "    audio_A = images[artist_A]\n",
    "    audio_B = images[artist_B]\n",
    "    \n",
    "    X_tmp = [list(), list()]\n",
    "    for pair in itertools.product(audio_A, audio_B):\n",
    "        X_tmp[0].append(pair[0])\n",
    "        X_tmp[1].append(pair[1])\n",
    "    \n",
    "    return np.mean(model.predict(X_tmp))\n",
    "\n",
    "def eval_artist_baseline(artist_A, artist_B):\n",
    "    # Similar to above, but using baseline evaluation of taking L2 distance between\n",
    "    # spectrograms and averaging\n",
    "    audio_A = images[artist_A]\n",
    "    audio_B = images[artist_B]\n",
    "    \n",
    "    dist = []\n",
    "    for pair in itertools.product(audio_A, audio_B):\n",
    "        dist.append(np.linalg.norm(pair[0]-pair[1]))\n",
    "    \n",
    "    return -np.mean(np.array(dist))\n",
    "\n",
    "def create_similarity_matrix(artists, model, all_artists = None, baseline = False):\n",
    "    # Creates a similarity matrix for a list of artists, using a given model to predict similarity\n",
    "    if all_artists is None:\n",
    "        all_artists = artists\n",
    "    df = pd.DataFrame(np.zeros((len(artists), len(all_artists))), columns = all_artists, index = artists)\n",
    "    for artist_A, artist_B in itertools.product(all_artists, artists):\n",
    "        if artist_A != artist_B:\n",
    "            if baseline:\n",
    "                sim = eval_artist_baseline(artist_A, artist_B)\n",
    "            else:\n",
    "                sim = eval_artist_similarity(artist_A, artist_B, model)\n",
    "            df[artist_A][artist_B] = sim\n",
    "    return df\n",
    "\n",
    "def compare_accuracy_edit_dist(df1, df2, n=10):\n",
    "    # Compares accuracy of two similarity ranking dataframes using edit distance, comparing the top n ranked\n",
    "    # similar artists\n",
    "    distances = dict()\n",
    "    for column in df1:\n",
    "        distances[column] = difflib.SequenceMatcher(None, df1.head(n)[column], df2.head(n)[column]).ratio()\n",
    "    return np.mean(np.array(list(distances.values()))), distances\n",
    "\n",
    "def compare_accuracy_unordered(df1, df2, n=10):\n",
    "    # Compares accuracy of two similarity ranking dataframes by looking at the size of the unordered union of sets,\n",
    "    # comparing the top n ranked similar artists\n",
    "    distances = dict()\n",
    "    for column in df1:\n",
    "        distances[column] = len(set(df1.head(n)[column].values).intersection(df2.head(n)[column].values))\\\n",
    "            / len(df1.head(n)[column])\n",
    "    return np.mean(np.array(list(distances.values()))), distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test datasets relating image references to values\n",
    "\n",
    "Previously, we had the large raw dataset which is of the form `df[artistA][artistB]=similarity`. Here, we create X and Y arrays, with the X array being a list of two arrays, each containing references to one of the spectrogram images that was loaded for a given artist. The Y array is a list containing the similarity scores for the corresponding two artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-formatting datasets for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d0c3c0743b4390b6c18615dec422ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=384126), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7db14482a8a441d99e0b14dbdf14184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46504), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets formatted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Re-formatting datasets for training')\n",
    "\n",
    "train_dedup = train_set.where(~np.triu(np.ones(train_set.shape)).astype(np.bool))\n",
    "train_stacked = train_dedup.stack()\n",
    "\n",
    "test_set.index = test_set.columns.append(test_set.columns.append(test_set.index).drop_duplicates(keep=False))\n",
    "test_dedup = test_set.where(~np.triu(np.ones(test_set.shape)).astype(np.bool))\n",
    "test_stacked = test_dedup.stack()\n",
    "\n",
    "def format_dataset(raw_dataset):\n",
    "    X_1 = []\n",
    "    X_2 = []\n",
    "    Y = []\n",
    "    \n",
    "    for index, value in tqdm_notebook(raw_dataset.iteritems(), total = len(raw_dataset)):\n",
    "        artist_A = index[0]\n",
    "        artist_B = index[1]\n",
    "        audio_A = images[artist_A]\n",
    "        audio_B = images[artist_B]\n",
    "\n",
    "        for pair in itertools.product(audio_A, audio_B):\n",
    "            X_1.append(pair[0])\n",
    "            X_2.append(pair[1])\n",
    "            Y.append(value)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    X = [X_1, X_2]\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = format_dataset(train_stacked)\n",
    "X_test, Y_test = format_dataset(test_stacked)\n",
    "\n",
    "logger.info('Datasets formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras model specification\n",
    "\n",
    "This defines a siamese network, which trains the same model with the same parameters and applies it to both images. The output of this shared vector is a fully-connected network with 128 neurons, for each image. The L1 distance between these two networks is then taken and the resulting 128 length vector is fed into a final sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_distance(x):\n",
    "    return K.abs(x[0] - x[1])\n",
    "\n",
    "def L1_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return shape1\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape = input_shape)\n",
    "    x = ZeroPadding1D()(input) # 258x64\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 256x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(4)(x) # 64x128\n",
    "    x = ZeroPadding1D()(x) # 66x128\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 64x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D()(x) # 32x128\n",
    "    x = ZeroPadding1D()(x) # 34x128\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 32x128\n",
    "    x = BatchNormalization()(x) \n",
    "    x = GlobalMaxPooling1D()(x) # 128\n",
    "    x = Dense(128, activation='relu')(x) # 128\n",
    "    return Model(input, x)\n",
    "    \n",
    "def model_spec(lr = 0.001, decay = 0.0, **kwargs):\n",
    "    input_shape = (256,64)\n",
    "\n",
    "    siamese_net = create_base_network(input_shape)\n",
    "\n",
    "    input_a = Input(shape = input_shape)\n",
    "    input_b = Input(shape = input_shape)\n",
    "\n",
    "    process_a = siamese_net(input_a)\n",
    "    process_b = siamese_net(input_b)\n",
    "\n",
    "    distance = Lambda(L1_distance, output_shape = L1_dist_output_shape)([process_a, process_b])\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "    model = Model([input_a, input_b], output)\n",
    "\n",
    "    adam = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = adam)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions\n",
    "\n",
    "Defines a function for training the model and evaluating it, as well as a function for generating batches (to avoid memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(X_data, Y_data, batch_size):\n",
    "    curr_batch = 0\n",
    "    while True:\n",
    "        end = min(curr_batch + batch_size, len(Y_data))\n",
    "        X = [np.asarray(X_data[0][curr_batch:end]), np.asarray(X_data[1][curr_batch:end])]\n",
    "        Y = np.asarray(Y_data[curr_batch:end])\n",
    "        yield (X, Y)\n",
    "        if end == len(Y_data):\n",
    "            curr_batch = 0\n",
    "        else:\n",
    "            curr_batch = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hparams, model_spec, log=True, save=True, test_accs = [3, 5, 10, 25]):\n",
    "    # Make the path to the logger/model directory, named based on parameters\n",
    "    path = os.path.join('models', 'full')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Get temporary logger with hparams.__str__ name \n",
    "    tmp_logger = make_logger(str(hparams), os.path.join(path, 'training.log'), )\n",
    "    \n",
    "    batch_size = hparams.get('batch_size', 128)\n",
    "    epochs = hparams.get('epochs', 25)\n",
    "    model = model_spec(**hparams)\n",
    "    \n",
    "    # Tensorboard support\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(path, 'tensorboard'))\n",
    "    \n",
    "    # CSV Logging of epoch results\n",
    "    csvlogger = CSVLogger(filename=os.path.join(path, 'epochs.csv'))\n",
    "\n",
    "    tmp_logger.info('Training with params {}'.format(hparams))\n",
    "    \n",
    "    # Define a callback to save the model at every epoch\n",
    "    filename = \"model-{epoch:02d}-{val_loss:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(os.path.join(path, filename), monitor='val_loss', verbose=0, \n",
    "                                 save_best_only=False, mode='auto', period=1)\n",
    "\n",
    "    history = model.fit_generator(batch_loader(X_train, Y_train, batch_size), epochs=epochs, \n",
    "                                  validation_data=batch_loader(X_test, Y_test, batch_size), \n",
    "                                  steps_per_epoch=np.ceil(len(Y_train) / batch_size), \n",
    "                                  validation_steps=np.ceil(len(Y_test) / batch_size),\n",
    "                                  callbacks=[checkpoint, tensorboard, csvlogger])\n",
    "    \n",
    "    tmp_logger.info('Finished training, final train loss = {:.5f}, test loss = {:.5f}'.format(\n",
    "        history.history['loss'][-1], history.history['val_loss'][-1]))\n",
    "\n",
    "    with open(os.path.join(path, 'history'), 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    \n",
    "    if save:\n",
    "        tmp_logger.info('Saving model')\n",
    "        model.save(os.path.join(path, 'post_train_model.hd5'))\n",
    "        tmp_logger.info('Saved model')\n",
    "        \n",
    "    return history.history, model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "This is it! Train the model using the optimal parameters we found earlier, i.e. 1024 batch size and 0.001 learning rate. We'll go for however many epochs as possible (up to 25), since we're saving the model at each epoch anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training final model\n",
      "Training with params {'batch_size': 1024, 'epochs': 25, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3083/3083 [==============================] - 1308s 424ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 2/25\n",
      "3083/3083 [==============================] - 1313s 426ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 3/25\n",
      "3083/3083 [==============================] - 1310s 425ms/step - loss: 0.0034 - val_loss: 0.0048\n",
      "Epoch 4/25\n",
      "3083/3083 [==============================] - 1298s 421ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 5/25\n",
      "3083/3083 [==============================] - 1290s 418ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 6/25\n",
      "2126/3083 [===================>..........] - ETA: 6:12 - loss: 0.0031"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 25, \n",
    "    'lr': 0.001\n",
    "}\n",
    "\n",
    "logger.info('Training final model')\n",
    "history, model = train_model(hparams, model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('post_train_model_x2.hd5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
