{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Dev Notebook\n",
    "\n",
    "This notebook is for development of a suitable neural network for training on the training set. Here, I use the dev set and tune hyperparameters and model structure for best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten, Lambda, BatchNormalization\n",
    "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_json, make_logger\n",
    "import logging\n",
    "import itertools\n",
    "import pickle\n",
    "import difflib\n",
    "\n",
    "params = load_json('params.json')\n",
    "logger = make_logger('development', 'log/development.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dev set and images\n",
    "\n",
    "Creates a dict relating artist MBIDs to spectrogram images, represented as 128x128x1 arrays of floats. Also load the previously saved database of artist relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2791), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading images')\n",
    "\n",
    "images = defaultdict(list)\n",
    "\n",
    "for path in tqdm_notebook(glob.glob('tracks/**/*.png', recursive=True)):\n",
    "    mbid = os.path.basename(path).rsplit('-', 1)[0]\n",
    "    img = np.rot90(np.array(Image.open(path).convert('L')) / 255)\n",
    "    #img = np.reshape(img, img.shape + (1,))\n",
    "    images[mbid].append(img)\n",
    "    \n",
    "logger.info('Loaded images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dev set\n",
      "Loaded dev set\n"
     ]
    }
   ],
   "source": [
    "logger.info('Loading dev set')\n",
    "\n",
    "raw_dataset = pd.read_hdf('dataset/dev_min.hd5', key='artists')\n",
    "\n",
    "logger.info('Loaded dev set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Artists in images: 975\n",
      "Dataset size: (49, 49)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Artists in images: {}'.format(len(images)))\n",
    "logger.info('Dataset size: {}'.format(raw_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define accuracy metric\n",
    "\n",
    "For my accuracy metric, I get the top similar artists for each artist from the original dataset, then to calculate accuracy of the model, I use the model to calculate all pairwise similarity scores and get the top predicted similar artists. Then I compare the two using edit distance, and return the average value as an accuracy.\n",
    "\n",
    "Calculating accuracy like this is slow because you pretty much have to go through a full epoch. It's probably only useful at the end of training.\n",
    "\n",
    "There are two accuracy metrics I use, all of which mean slightly different things:\n",
    "\n",
    "`compare_accuracy_edit_dist` compares the top n artists in each dataframe by edit distance, and returns it as a proportion to the maximum edit distance possible (i.e. n)\n",
    "\n",
    "`compare_accuracy_unordered` compares how many of the top n artists are shared between both dataframes, disregarding order, as a proportion of n. \n",
    "\n",
    "Additionally, here I define a baseline which simply takes the L2 norm of the difference between the two tracks, averaging to get the similarity score. This typically performs better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_to_top(matrix):\n",
    "    # Converts a similarity score matrix to a sorted table of most similar artists\n",
    "    most_similar = np.argsort(-matrix.values, axis=1)\n",
    "    return pd.DataFrame(most_similar, index=matrix.index).T\n",
    "\n",
    "def eval_artist_similarity(artist_A, artist_B, model):\n",
    "    # Collect all pairwise combinations of artist tracks, evaluates the model on them, then\n",
    "    # returns the similarity as an average of all the predictions\n",
    "    audio_A = images[artist_A]\n",
    "    audio_B = images[artist_B]\n",
    "    \n",
    "    X_tmp = [list(), list()]\n",
    "    for pair in itertools.product(audio_A, audio_B):\n",
    "        X_tmp[0].append(pair[0])\n",
    "        X_tmp[1].append(pair[1])\n",
    "    \n",
    "    return np.mean(model.predict(X_tmp))\n",
    "\n",
    "def eval_artist_baseline(artist_A, artist_B):\n",
    "    # Similar to above, but using baseline evaluation of taking L2 distance between\n",
    "    # spectrograms and averaging\n",
    "    audio_A = images[artist_A]\n",
    "    audio_B = images[artist_B]\n",
    "    \n",
    "    dist = []\n",
    "    for pair in itertools.product(audio_A, audio_B):\n",
    "        dist.append(np.linalg.norm(pair[0]-pair[1]))\n",
    "    \n",
    "    return -np.mean(np.array(dist))\n",
    "\n",
    "def create_similarity_matrix(artists, model, all_artists = None, baseline = False):\n",
    "    # Creates a similarity matrix for a list of artists, using a given model to predict similarity\n",
    "    if all_artists is None:\n",
    "        all_artists = artists\n",
    "    df = pd.DataFrame(np.zeros((len(artists), len(all_artists))), columns = all_artists, index = artists)\n",
    "    for artist_A, artist_B in itertools.product(all_artists, artists):\n",
    "        if artist_A != artist_B:\n",
    "            if baseline:\n",
    "                sim = eval_artist_baseline(artist_A, artist_B)\n",
    "            else:\n",
    "                sim = eval_artist_similarity(artist_A, artist_B, model)\n",
    "            df[artist_A][artist_B] = sim\n",
    "    return df\n",
    "\n",
    "def compare_accuracy_edit_dist(df1, df2, n=10):\n",
    "    # Compares accuracy of two similarity ranking dataframes using edit distance, comparing the top n ranked\n",
    "    # similar artists\n",
    "    distances = dict()\n",
    "    for column in df1:\n",
    "        distances[column] = difflib.SequenceMatcher(None, df1.head(n)[column], df2.head(n)[column]).ratio()\n",
    "    return np.mean(np.array(list(distances.values()))), distances\n",
    "\n",
    "def compare_accuracy_unordered(df1, df2, n=10):\n",
    "    # Compares accuracy of two similarity ranking dataframes by looking at the size of the unordered union of sets,\n",
    "    # comparing the top n ranked similar artists\n",
    "    distances = dict()\n",
    "    for column in df1:\n",
    "        distances[column] = len(set(df1.head(n)[column].values).intersection(df2.head(n)[column].values))\\\n",
    "            / len(df1.head(n)[column])\n",
    "    return np.mean(np.array(list(distances.values()))), distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into dev-train and dev-test sets\n",
    "\n",
    "For better validation, split the dev set 75:25 into a dev-train and dev-test set. As how we generated the original train/test split, we will split based on artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting dev dataset into dev-train and dev-test sets\n"
     ]
    }
   ],
   "source": [
    "logger.info('Splitting dev dataset into dev-train and dev-test sets')\n",
    "\n",
    "allartists = raw_dataset.index\n",
    "\n",
    "train_artists, test_artists = train_test_split(allartists, test_size=0.25, random_state = 1)\n",
    "\n",
    "train_set = raw_dataset.loc[train_artists, train_artists]\n",
    "test_set = raw_dataset.loc[test_artists]\n",
    "test_set_ranked = similarity_matrix_to_top(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset relating image references to values\n",
    "\n",
    "Previously, we had the large raw dataset which is of the form `raw_dataset[artistA][artistB]=similarity`. Here, we create X and Y arrays, with the X array being a list of two arrays, each containing references to one of the spectrogram images that was loaded for a given artist. The Y array is a list containing the similarity scores for the corresponding two artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-formatting datasets for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets formatted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Re-formatting datasets for training')\n",
    "\n",
    "train_dedup = train_set.where(~np.triu(np.ones(train_set.shape)).astype(np.bool))\n",
    "train_stacked = train_dedup.stack()\n",
    "\n",
    "test_dedup = test_set.where(~np.triu(np.ones(test_set.shape)).astype(np.bool))\n",
    "test_stacked = test_dedup.stack()\n",
    "\n",
    "def format_dataset(raw_dataset):\n",
    "    X_1 = []\n",
    "    X_2 = []\n",
    "    Y = []\n",
    "    \n",
    "    for index, value in tqdm_notebook(raw_dataset.iteritems(), total = len(raw_dataset)):\n",
    "        artist_A = index[0]\n",
    "        artist_B = index[1]\n",
    "        audio_A = images[artist_A]\n",
    "        audio_B = images[artist_B]\n",
    "\n",
    "        for pair in itertools.product(audio_A, audio_B):\n",
    "            X_1.append(pair[0])\n",
    "            X_2.append(pair[1])\n",
    "            Y.append(value)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    X = [X_1, X_2]\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = format_dataset(train_stacked)\n",
    "X_test, Y_test = format_dataset(test_stacked)\n",
    "\n",
    "logger.info('Datasets formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras model specification\n",
    "\n",
    "This defines a siamese network, which trains the same model with the same parameters and applies it to both images. The output of this shared vector is a fully-connected network with 512 neurons, for each image. The L1 distance between these two networks is then taken and the resulting 512 length vector is fed into a final sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_distance(x):\n",
    "    return K.abs(x[0] - x[1])\n",
    "\n",
    "def L1_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return shape1\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape = input_shape)\n",
    "    x = ZeroPadding1D()(input) # 258x64\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 256x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(4)(x) # 64x128\n",
    "    x = ZeroPadding1D()(x) # 66x128\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 64x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D()(x) # 32x128\n",
    "    x = ZeroPadding1D()(x) # 34x128\n",
    "    x = Conv1D(128,3,activation='relu')(x) # 32x128\n",
    "    x = BatchNormalization()(x) \n",
    "    x = GlobalMaxPooling1D()(x) # 128\n",
    "    x = Dense(128, activation='relu')(x) # 128\n",
    "    return Model(input, x)\n",
    "    \n",
    "def model_spec(lr = 0.001, decay = 0.0, **kwargs):\n",
    "    input_shape = (256,64)\n",
    "\n",
    "    siamese_net = create_base_network(input_shape)\n",
    "\n",
    "    input_a = Input(shape = input_shape)\n",
    "    input_b = Input(shape = input_shape)\n",
    "\n",
    "    process_a = siamese_net(input_a)\n",
    "    process_b = siamese_net(input_b)\n",
    "\n",
    "    distance = Lambda(L1_distance, output_shape = L1_dist_output_shape)([process_a, process_b])\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "    model = Model([input_a, input_b], output)\n",
    "\n",
    "    adam = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = adam)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "This function will take in a model and a hyperparameters dict, and output training and test losses after training. The following hyperparameters are implemented:\n",
    "\n",
    "`batch_size`: batch size for training\n",
    "\n",
    "`epochs`: number of epochs to use in the training phase\n",
    "\n",
    "`lr`: learning rate for model\n",
    "\n",
    "`decay`: decay for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(X_data, Y_data, batch_size):\n",
    "    curr_batch = 0\n",
    "    while True:\n",
    "        end = min(curr_batch + batch_size, len(Y_data))\n",
    "        X = [np.asarray(X_data[0][curr_batch:end]), np.asarray(X_data[1][curr_batch:end])]\n",
    "        Y = np.asarray(Y_data[curr_batch:end])\n",
    "        yield (X, Y)\n",
    "        if end == len(Y_data):\n",
    "            curr_batch = 0\n",
    "        else:\n",
    "            curr_batch = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_path(hparams):\n",
    "    path_components = ['_'.join([key, str(val)]) for key,val in hparams.items()]\n",
    "    return os.path.join('models', *path_components)\n",
    "\n",
    "def test_hyperparams(hparams, model_spec, log=True, save=True, test_accs = [3, 5, 10, 25]):\n",
    "    # Make the path to the logger/model directory, named based on parameters\n",
    "    path = build_model_path(hparams)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Get temporary logger with hparams.__str__ name \n",
    "    tmp_logger = make_logger(str(hparams), os.path.join(path, 'training.log'), )\n",
    "    \n",
    "    batch_size = hparams.get('batch_size', 128)\n",
    "    epochs = hparams.get('epochs', 25)\n",
    "    model = model_spec(**hparams)\n",
    "\n",
    "    tmp_logger.info('Training with params {}'.format(hparams))\n",
    "\n",
    "    history = model.fit_generator(batch_loader(X_train, Y_train, batch_size), epochs=epochs, \n",
    "                                  validation_data=batch_loader(X_test, Y_test, batch_size), \n",
    "                                  steps_per_epoch=np.ceil(len(Y_train) / batch_size), \n",
    "                                  validation_steps=np.ceil(len(Y_test) / batch_size))\n",
    "    return model\n",
    "    sim_matrix = create_similarity_matrix(test_artists, model, all_artists=allartists)\n",
    "    edit_dist_accuracies = {}\n",
    "    unordered_accuracies = {}\n",
    "    \n",
    "    for acc in test_accs:\n",
    "        edit_dist_accuracy, _ = compare_accuracy_edit_dist(similarity_matrix_to_top(sim_matrix), test_set_ranked, n=acc)\n",
    "        unordered_accuracy, _ = compare_accuracy_unordered(similarity_matrix_to_top(sim_matrix), test_set_ranked, n=acc)\n",
    "        edit_dist_accuracies[acc] = edit_dist_accuracy\n",
    "        unordered_accuracies[acc] = unordered_accuracy\n",
    "\n",
    "    tmp_logger.info('Finished training, final train loss = {:.5f}, test loss = {:.5f}'.format(\n",
    "        history.history['loss'][-1], history.history['val_loss'][-1]))\n",
    "        \n",
    "    tmp_logger.info('Edit Distance Accuracy: {}'.format(edit_dist_accuracies))\n",
    "    tmp_logger.info('Unordered Accuracy: {}'.format(unordered_accuracies))\n",
    "    \n",
    "    with open(os.path.join(path, 'history'), 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "        \n",
    "    with open(os.path.join(path, 'edit_dist_acc.pickle'), 'wb') as file:\n",
    "        pickle.dump(edit_dist_accuracies, file)\n",
    "        \n",
    "    with open(os.path.join(path, 'unordered_acc.pickle'), 'wb') as file:\n",
    "        pickle.dump(unordered_accuracies, file)\n",
    "    \n",
    "    if save:\n",
    "        tmp_logger.info('Saving model')\n",
    "        model.save(os.path.join(path, 'post_train_model.hd5'))\n",
    "        tmp_logger.info('Saved model')\n",
    "        \n",
    "    return history.history, edit_dist_accuracies, unordered_accuracies, model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through Hyperparameter sets and Evaluate\n",
    "\n",
    "Create a dict containing all values for all hyperparameters we want to look at, then generate all combinations of these parameters and test each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_hparams = {\n",
    "    'batch_size': [256, 512, 1024],\n",
    "    'epochs': [10, 25, 50], \n",
    "    'lr': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "hparams_list = list()\n",
    "keys, vals = zip(*all_hparams.items())\n",
    "for val in itertools.product(*vals):\n",
    "    hparams_list.append(dict(zip(keys, val)))\n",
    "\n",
    "logger.info('Generated hyperparameters test list, total of {} combinations'.format(len(hparams_list)))\n",
    "\n",
    "results = []\n",
    "\n",
    "for hparams in tqdm_notebook(hparams_list):\n",
    "    logger.info('Testing hyperparameter set {}'.format(hparams))\n",
    "    history, edit_dist_accs, unordered_accs = test_hyperparams(hparams, model_spec)\n",
    "    results.append((hparams, history, edit_dist_accs, unordered_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random testing stuff below\n",
    "\n",
    "Test a specific model, and evaluate accuracy with both the model and the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 1024, 'epochs': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0456 - val_loss: 0.0039\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 2s 426ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 2s 414ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 2s 413ms/step - loss: 0.0027 - val_loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 10\n",
    "}\n",
    "\n",
    "model = test_hyperparams(hparams, model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = create_similarity_matrix(test_artists, model, all_artists=allartists)\n",
    "sim_matrix_baseline = create_similarity_matrix(test_artists, model, all_artists=allartists, baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unordered accuracy; model: 0.10256410256410256, baseline: 0.10256410256410256. \n",
      "    Edit Distance Accuracy; model: 0.10256410256410256, baseline: 0.10256410256410256, Random unordered accuracy: 0.061224489795918366\n",
      "Unordered accuracy; model: 0.24615384615384622, baseline: 0.15384615384615385. \n",
      "    Edit Distance Accuracy; model: 0.18461538461538465, baseline: 0.13846153846153844, Random unordered accuracy: 0.10204081632653061\n",
      "Unordered accuracy; model: 0.38974358974358975, baseline: 0.34358974358974365. \n",
      "    Edit Distance Accuracy; model: 0.1794871794871795, baseline: 0.17948717948717946, Random unordered accuracy: 0.30612244897959184\n"
     ]
    }
   ],
   "source": [
    "sim_ranks = similarity_matrix_to_top(sim_matrix)\n",
    "sim_baseline_ranks = similarity_matrix_to_top(sim_matrix_baseline)\n",
    "\n",
    "for acc in [3, 5, 15]:\n",
    "    unord_model_acc, _ = compare_accuracy_unordered(sim_ranks, test_set_ranked, n=acc)\n",
    "    unord_base_acc, _ = compare_accuracy_unordered(sim_baseline_ranks, test_set_ranked, n=acc)\n",
    "    edit_model_acc, _ = compare_accuracy_edit_dist(sim_ranks, test_set_ranked, n=acc)\n",
    "    edit_base_acc, _ = compare_accuracy_edit_dist(sim_baseline_ranks, test_set_ranked, n=acc)\n",
    "    \n",
    "    print('''Unordered accuracy; model: {}, baseline: {}. \n",
    "    Edit Distance Accuracy; model: {}, baseline: {}, Random unordered accuracy: {}'''.format(\n",
    "        unord_model_acc,\n",
    "        unord_base_acc,\n",
    "        edit_model_acc,\n",
    "        edit_base_acc,\n",
    "        acc / len(allartists)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
